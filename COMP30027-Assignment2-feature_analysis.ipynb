{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets\n",
    "\n",
    "This is a sample code to assist you with vectorising the 'Train' dataset for your assignment 2.\n",
    "\n",
    "First we read the CSV datafiles (Train and Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Quang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preprocessing\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# # plotting\n",
    "# import seaborn as sns\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# # sklearn\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# X_train=train_data['text'].str.lower()\n",
    "\n",
    "# X_train.tail()\n",
    "\n",
    "# # remove urls\n",
    "# def cleaning_URLs(data):\n",
    "#     return re.sub(r'((www.[^s]+)|(https?://[^s]+)|(https?://[^s]+))',' ',data)\n",
    "# X_train  = X_train .apply(lambda x: cleaning_URLs(x))\n",
    "# X_train.tail()\n",
    "\n",
    "\n",
    "\n",
    "# # remove stop words\n",
    "# stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an','and','any','are', 'as', 'at', 'be', 'because', 'been', 'before','being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do','does' 'doing', 'down', 'during', 'each','few', 'for', 'from','further', 'had', 'has', 'have', 'having', 'he', 'her', 'here','hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in','into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma','me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once', 'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such','t', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those','through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was','we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom','why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "# STOPWORDS = set(stopwordlist)\n",
    "# def cleaning_stopwords(text):\n",
    "#     return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "# X_train = X_train.apply(lambda text: cleaning_stopwords(text))\n",
    "\n",
    "# # remove punctuation\n",
    "# import string\n",
    "# english_punctuations = string.punctuation\n",
    "# punctuations_list = english_punctuations\n",
    "# def cleaning_punctuations(text):\n",
    "#     translator = str.maketrans('', '', punctuations_list)\n",
    "#     return text.translate(translator)\n",
    "# X_train= X_train.apply(lambda x: cleaning_punctuations(x))\n",
    "# print(X_train.head(5))\n",
    "\n",
    "# # remove repeating characters\n",
    "# def cleaning_repeating_char(text):\n",
    "#     return re.sub(r'(.)1+', r'1', text)\n",
    "# X_train = X_train.apply(lambda x: cleaning_repeating_char(x))\n",
    "# X_train.tail()\n",
    "\n",
    "# # remove numbers\n",
    "# def cleaning_numbers(data):\n",
    "#     return re.sub(r'[0-9]+', '', data)\n",
    "# X_train = X_train.apply(lambda x: cleaning_numbers(x))\n",
    "# X_train = X_train.to_frame()\n",
    "\n",
    "# X_train_raw = [x[0] for x in X_train[['text']].values]\n",
    "\n",
    "\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer(r'w+')\n",
    "# X_train = X_train.apply(tokenizer.tokenize)\n",
    "# print(X_train[100])\n",
    "# X_train.tail()\n",
    "\n",
    "# import nltk\n",
    "# st = nltk.PorterStemmer()\n",
    "# def stemming_on_text(data):\n",
    "#     text = [st.stem(word) for word in data]\n",
    "#     return data\n",
    "# X_train= X_train.apply(lambda x: stemming_on_text(x))\n",
    "# X_train.head()\n",
    "\n",
    "# lm = nltk.WordNetLemmatizer()\n",
    "# def lemmatizer_on_text(data):\n",
    "#     text = [lm.lemmatize(word) for word in data]\n",
    "#     return data\n",
    "# X_train = X_train.apply(lambda x: lemmatizer_on_text(x))\n",
    "# X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we separate the tweet text and the label (sentiment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 21802\n",
      "Test length: 6099\n"
     ]
    }
   ],
   "source": [
    "#separating instance and label for Train\n",
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "Y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "\n",
    "#check the result\n",
    "print(\"Train length:\",len(X_train_raw))\n",
    "\n",
    "#separating instance and label for Test\n",
    "X_test_raw = [x[0] for x in test_data[['text']].values]\n",
    "\n",
    "#separating id for Test\n",
    "X_test_id = [x[0] for x in test_data[['id']].values]\n",
    "\n",
    "#check the result\n",
    "print(\"Test length:\",len(X_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = [x[0] for x in train_data[['text']].values]\n",
    "X_train_raw = [item.lower() for item in X_train_raw]\n",
    "X_train_raw = [re.sub('((www.[^s]+)|(https?://[^s]+))',' ',item) for item in X_train_raw]\n",
    "X_train_raw = [re.sub('[^a-z ]+', \"\", item) for item in X_train_raw]\n",
    "\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "STOPWORDS = set(stopwordlist)\n",
    "X_train_raw = [\" \".join([word for word in str(item).split() if word not in STOPWORDS]) for item in X_train_raw]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "X_train_raw = [tokenizer.tokenize(item) for item in X_train_raw]\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "def stem_string(arr):\n",
    "    arr = [ps.stem(item) for item in arr]\n",
    "    return arr\n",
    "X_train_raw = [stem_string(item) for item in X_train_raw]\n",
    "\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatize_string(arr):\n",
    "    arr = [lm.lemmatize(item) for item in arr]\n",
    "    return arr\n",
    "\n",
    "X_train_raw = [lemmatize_string(item) for item in X_train_raw]\n",
    "X_train_raw = [' '.join(item) for item in X_train_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anybodi go radio station tomorrow see shawn friend may go but would like make new friendsmeet\n"
     ]
    }
   ],
   "source": [
    "#Let's see one example tweet\n",
    "print(X_train_raw[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words (BoW)\n",
    "In this approach, we use the **CountVectorizer** library to separate all the words in the Train corpus (dataset). These words are then used as the 'vectors' or 'features' to represent each instance (Tweet) in `Train` and `Test` datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space size (using BoW): (21802, 31181)\n",
      "Test feature space size (using BoW): (6099, 31181)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BoW_vectorizer = CountVectorizer()\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Train dataset using BoW\n",
    "X_train_BoW = BoW_vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "#Use the feature set (vocabulary) from Train to vectorise the Test dataset \n",
    "X_test_BoW = BoW_vectorizer.transform(X_test_raw)\n",
    "\n",
    "print(\"Train feature space size (using BoW):\",X_train_BoW.shape)\n",
    "print(\"Test feature space size (using BoW):\",X_test_BoW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row is a list of tuples with the vector_id (word_id in the vocabulary) and the number of times it repeated in that given instance (tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1205)\t1\n",
      "  (0, 9908)\t2\n",
      "  (0, 20340)\t1\n",
      "  (0, 25329)\t1\n",
      "  (0, 28127)\t1\n",
      "  (0, 22624)\t1\n",
      "  (0, 23146)\t1\n",
      "  (0, 9220)\t1\n",
      "  (0, 15623)\t1\n",
      "  (0, 3650)\t1\n",
      "  (0, 30586)\t1\n",
      "  (0, 14538)\t1\n",
      "  (0, 15214)\t1\n",
      "  (0, 17371)\t1\n",
      "  (0, 9226)\t1\n"
     ]
    }
   ],
   "source": [
    "#Let's see one example tweet using the BoW feature space\n",
    "print(X_train_BoW[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the created vocabulary for the given dataset in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = BoW_vectorizer.vocabulary_\n",
    "output_pd = pd.DataFrame(list(output_dict.items()),columns = ['word','count'])\n",
    "\n",
    "output_pd.T.to_csv('BoW-vocab.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TFIDF\n",
    "In this approach, we use the **TfidfVectorizer** library to separate all the words in this corpus (dataset). Same as the BoW approach, these words are then used as the 'vectors' or 'features' to represent each instance (Tweet).\n",
    "\n",
    "However, in this method for each instance the value associated with each 'vector' (word) is not the number of times the word repeated in that tweet, but the TFIDF value of then 'voctor' (word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Build the feature set (vocabulary) and vectorise the Tarin dataset using TFIDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "#Use the feature set (vocabulary) from Train to vectorise the Test dataset \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "\n",
    "# print(\"Train feature space size (using TFIDF):\",X_train_BoW.shape)\n",
    "# print(\"Test feature space size (using TFIDF):\",X_test_BoW.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9226)\t0.4471814697109317\n",
      "  (0, 17371)\t0.19289699614243974\n",
      "  (0, 15214)\t0.1942236816102367\n",
      "  (0, 14538)\t0.17174454464915798\n",
      "  (0, 30586)\t0.2091585381468773\n",
      "  (0, 3650)\t0.15495862301487676\n",
      "  (0, 15623)\t0.1407707994292212\n",
      "  (0, 9220)\t0.25553461961314927\n",
      "  (0, 23146)\t0.24986718607333058\n",
      "  (0, 22624)\t0.17483383447481923\n",
      "  (0, 28127)\t0.1487715003874696\n",
      "  (0, 25329)\t0.34940836675605325\n",
      "  (0, 20340)\t0.3110076520928755\n",
      "  (0, 9908)\t0.3220517114870673\n",
      "  (0, 1205)\t0.32413564324231986\n"
     ]
    }
   ],
   "source": [
    "#Let's see one example tweet using the TFIDF feature space\n",
    "print(X_train_tfidf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model 0R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     12659\n",
      "positive     5428\n",
      "negative     3715\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Build 0R\n",
    "\n",
    "#split training dataset into 3 class, possitive, negative and neutral\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "positive_set = train_data [(train_data [\"sentiment\"] == 'positive')]\n",
    "neutral_set = train_data [(train_data [\"sentiment\"] == 'neutral')]\n",
    "negative_set = train_data [(train_data [\"sentiment\"] == 'negative')]\n",
    "print(train_data[\"sentiment\"].value_counts())\n",
    "\n",
    "#find the class with the most instance\n",
    "max_size = len(positive_set)\n",
    "max_dataset = positive_set\n",
    "for data_set in [neutral_set, negative_set]:\n",
    "    if len(data_set) > max_size:\n",
    "        max_dataset = data_set\n",
    "        max_size = len(data_set)\n",
    "\n",
    "# the model will use the class with the most instance to classify all of the test data\n",
    "chosen_class = max_dataset.iloc[0][\"sentiment\"]\n",
    "\n",
    "# classify test set\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n",
    "test_data['sentiment'] = chosen_class\n",
    "(test_data.drop(['text'], axis=1)).to_csv('base.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "BNBmodel = BernoulliNB()\n",
    "BNBmodel.fit(X_train_tfidf, Y_train)\n",
    "y_pred = BNBmodel.predict(X_test_tfidf)\n",
    "\n",
    "data = {'id':X_test_id,'text':X_test_raw,'sentiment':y_pred}\n",
    "predictions = pd.DataFrame(data=data)\n",
    "(predictions.drop(['text'], axis=1)).to_csv('bnbmodel.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "SVMmodel = LinearSVC()\n",
    "SVMmodel.fit(X_train_tfidf, Y_train)\n",
    "y_pred2 = SVMmodel.predict(X_test_tfidf)\n",
    "\n",
    "data = {'id':X_test_id,'text':X_test_raw,'sentiment':y_pred2}\n",
    "predictions = pd.DataFrame(data=data)\n",
    "(predictions.drop(['text'], axis=1)).to_csv('svmmodel.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "random_forest_model=RandomForestClassifier(n_estimators=100)\n",
    "random_forest_model.fit(X_train_tfidf, Y_train)\n",
    "y_pred3 = random_forest_model.predict(X_test_tfidf)\n",
    "\n",
    "data = {'id':X_test_id,'text':X_test_raw,'sentiment':y_pred3}\n",
    "predictions = pd.DataFrame(data=data)\n",
    "(predictions.drop(['text'], axis=1)).to_csv('rfmodel.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
